---
title: "假設與研究設計"
author: "陳紹慶"
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## 調查目標

1. 接受分類詞片語的人數比例
2. 母語者經驗認定可搭配目標分類詞之實體物件名詞

## 假設與問題

第一階段預先註冊pdf檔[製作中]。

### 確證性假設

1A. 中性化量詞「個」之分類詞片語接受度，與表徵立體物件的分類詞片語接受度（「顆、粒」）呈現一致的正相關性。

1B. 中性化量詞「個」之分類詞片語接受度，與表徵一維及平面物件的分類詞片語接受度（「條、根、枝、張、面、片」）呈現不一致的負相關性。

2. 田意民等人(2002)選擇的分類詞及的MDS分佈，符合一維、平面及立體特徵的群集。

### 探索性問題

1. 新選的分類詞片語接受度，與表徵同一類物件形狀的分類詞片語接受度，有一致的正相關性。

2. 新選的分類詞MDS分佈，會符合一維、平面及立體特徵的群集。

3. 請參與者自行報告未列在調查問題，依經驗能與提示的分類詞搭配的實體物件名詞。自取語料有可能發現這些名詞。

## 問卷設計

問卷一共有五種版本，每個版本都有田意民等人(2002)選用的9項分類詞與90個名詞，各版本增加"幅","具","塊","枚","座"其中一項分類詞及平衡語料庫取出的10個搭配名詞。每個版本都有10項分詞與100個名詞組合的1000條分類詞片語，分類詞片語的呈現格式如「一片 白雲」。
所有分類詞片語以分頁隨機的逐題呈現，參與者將逐題判斷是否接受每條分類詞片語的組成。
1000條判斷題結束之後，參與者再逐一對每個分類詞，回答曾在問卷內看過，最能接受的搭配名詞，以及回答不在問卷之內，但是可以搭配的物件名詞。

```{r sti_making, message=FALSE, warning=FALSE, include=FALSE}
## Define the classifiers from targeted study
THT_classifiers <- c("一條","一枝","一根","一張","一面","一片","一個","一顆","一粒")
## Define the new classifiers for this study
NEW_classifiers <- c("一幅","一具","一塊","一枚","一座")

## Import the old classifiers and the inspected nouns
THT_sti <- read_csv(file="phase01_sinica_corpus/THT_nouns_check.csv",na = "NA")%>%filter(STUDY01 == "V") %>% select(WORD,POS,FREQ,RANK,`條`,`枝`,`根`,`張`,`面`,`片`,`個`,`顆`,`粒`) %>% pull(WORD)

## Import the selected nouns for the new classifiers
sti01 <- read_csv(file="phase01_sinica_corpus/classifiers_pool/sti_fu.csv") %>% filter(In < 11) %>% pull(WORD)

sti02 <- read_csv(file="phase01_sinica_corpus/classifiers_pool/sti_ju.csv") %>% filter(In < 11) %>% pull(WORD)

sti03 <- read_csv(file="phase01_sinica_corpus/classifiers_pool/sti_kuai.csv") %>% filter(In < 11) %>% pull(WORD)

sti04 <- read_csv(file="phase01_sinica_corpus/classifiers_pool/sti_mei.csv") %>% filter(In < 11) %>% pull(WORD)

sti05 <- read_csv(file="phase01_sinica_corpus/classifiers_pool/sti_zuo.csv") %>% filter(In < 11) %>% pull(WORD)

if(!file.exists("phase01_sti")) {dir.create("phase01_sti", showWarnings = FALSE)}

## Making of stimuli lists
sti_set01 <- paste(rep(c(THT_classifiers,NEW_classifiers[1]),each=100),c(THT_sti,sti01), collapse = "\",\"") %>% write_lines(file="phase01_sti/sti_list01.txt")

sti_set02 <- paste(rep(c(THT_classifiers,NEW_classifiers[2]),each=100),c(THT_sti,sti02), collapse = "\",\"") %>% write_lines(file="phase01_sti/sti_list02.txt")

sti_set03 <- paste(rep(c(THT_classifiers,NEW_classifiers[3]),each=100),c(THT_sti,sti03), collapse = "\",\"") %>% write_lines(file="phase01_sti/sti_list03.txt")

sti_set04 <- paste(rep(c(THT_classifiers,NEW_classifiers[4]),each=100),c(THT_sti,sti04), collapse = "\",\"") %>% write_lines(file="phase01_sti/sti_list04.txt")

sti_set05 <- paste(rep(c(THT_classifiers,NEW_classifiers[5]),each=100),c(THT_sti,sti05), collapse = "\",\"") %>% write_lines(file="phase01_sti/sti_list05.txt")
```

題目清單可由以下連結下載：

- [題本01](phase01_sti/sti_list01.txt)
- [題本02](phase01_sti/sti_list02.txt)
- [題本03](phase01_sti/sti_list03.txt)
- [題本04](phase01_sti/sti_list04.txt)
- [題本05](phase01_sti/sti_list05.txt)


## 問卷部署及參與者來源

方案一：

所有問卷將透過塔拓問卷代發平台( https://www.tatoh.com.tw/ )部署。

方案二：

自製網路問卷，透過賞金達人 ( https://www.bountyworkers.net/ ) 發佈。


參與者背景

收集台灣各地區參與者回覆，參與者必須是中文母語人士，小學至高中都在台灣地區完成教育。每個版本預定收集至少40位參與者。

下圖是以google form製作的問卷題目範本：

![](img/form_template.png)

## 問卷資料處理

```{r pseudo_data, include=FALSE}
## Making of pseudo data
## Load items
items_set01 <- paste(rep(c(THT_classifiers,NEW_classifiers[1]),each=100),c(THT_sti,sti01))
write_csv(
    data.frame(## Pseudo ID of 40 participants
ID = rep(paste0("s",sprintf("%03d",1:40)),each=1000),
Items = rep(items_set01,40),
## Pseudo responses of 25 participants
Response = apply(matrix(rep(c(0,1),1000*40),nc=2,byrow = TRUE),1,sample,size=1)
)
    ,file = "phase01_data/pseudo_responses01.csv")
```

任何版本的題本收集到的反應資料，如同[虛擬檔案](phase01_data/pseudo_responses01.csv)的內容。


## 問卷資料分析

根據假設1A，總計每位參與者對各項分類詞之接受比率，計算各分類詞之間的相關係數。每個題本的資料可產生如以下的相關係數矩陣。

```{r pseudo_cor}
data001 <- read_csv(file = "phase01_data/pseudo_responses01.csv")
classifier_summary <- data001 %>%
    separate(Items,c("Classifier","Noun"),sep = " ") %>%
    unite("ID_Item",ID:Classifier, sep = " ") %>%
    group_by(ID_Item) %>%
    summarise(Prop = mean(Response)) %>%
    separate(ID_Item,c("ID","Classifier"),sep = " ") %>%
    as_tibble() %>%
    pivot_wider(names_from = "Classifier",values_from = "Prop")
cor(classifier_summary[,-1]) %>% as_tibble()
```

